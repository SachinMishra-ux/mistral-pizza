# -*- coding: utf-8 -*-
"""Mistral7llamaindexDemo1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AzqnFt5ZVV5lOImaJ73clJ3gq-Q0Onid
"""

!pip install -q pypdf
!pip install -q python-dotenv

from google.colab import drive
drive.mount('/content/drive')

!pip install -q transformers

!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install  llama-cpp-python --no-cache-dir

!pip install -q llama-index

import logging
import sys

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))
from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext

documents = SimpleDirectoryReader("/content/drive/MyDrive/Pizza_LLM").load_data()

!pip install llama-index-llms-llama-cpp

import torch

from llama_index.llms.llama_cpp import LlamaCPP
from llama_index.llms.llama_cpp.llama_utils import (
    messages_to_prompt,
    completion_to_prompt,
)

llm = LlamaCPP(
    # You can pass in the URL to a GGML model to download it automatically
    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf',
    # optionally, you can set the path to a pre-downloaded model instead of model_url
    model_path=None,
    temperature=0.1,
    max_new_tokens=256,
    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room
    context_window=3900,
    # kwargs to pass to __call__()
    generate_kwargs={},
    # kwargs to pass to __init__()
    # set to at least 1 to use GPU
    model_kwargs={"n_gpu_layers": -1},
    # transform inputs into Llama2 format
    messages_to_prompt=messages_to_prompt,
    completion_to_prompt=completion_to_prompt,
    verbose=True,
)

!pip -q install sentence-transformers

!pip install llama-index-embeddings-langchain

!pip install langchain

from langchain.embeddings import HuggingFaceEmbeddings
from llama_index.embeddings.langchain import LangchainEmbedding

embed_model = LangchainEmbedding(
  HuggingFaceEmbeddings(model_name="thenlper/gte-large")
)

service_context = ServiceContext.from_defaults(
    chunk_size=256,
    llm=llm,
    embed_model=embed_model
)

index = VectorStoreIndex.from_documents(documents, service_context=service_context)

from llama_index.core.memory import ChatMemoryBuffer

memory = ChatMemoryBuffer.from_defaults(token_limit=1500)

chat_engine = index.as_chat_engine(
    chat_mode="context",
    memory=memory,
    system_prompt=(
        """You are a helpful pizza-bot. Follow this flow for taking the orders from customers.
        First greet the customer. Then ask for orders of pizza from customers & try to customize their order if they ask for customization & finally confirm thier order."""
        "Hi"
    ),
)

chat_engine = index.as_chat_engine()

response = chat_engine.chat("Hi there !")

print(response)

response = chat_engine.chat("I want to order a pizza")

print(response)

response = chat_engine.chat("Let's go for chicken golden delight ")

print(response)

response = chat_engine.chat("please add grilled mushrooms")

print(response)

response = chat_engine.chat("what drinks you have ?")

print(response)

memory = ChatMemoryBuffer.from_defaults(token_limit=1500)

chat_engine = index.as_chat_engine(
    chat_mode="context",
    memory=memory,
    system_prompt=(
        """You are a helpful pizza assistant named PizzaAroma. Follow this flow for taking the orders from customers.
        First greet the customer. Then ask for orders of pizza from customers & try to customize their order if they ask for customization & finally confirm thier order."""
    ),
)

while True:
  query=input()
  response = chat_engine.chat(query)
  print(response)

